{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1267593,"sourceType":"datasetVersion","datasetId":723383}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Brain Tumor Segmentation with MONAI U-Net and 3D Reconstruction\n\n**Quick overview**  \nThis notebook implements an end-to-end pipeline to automatically segment brain tumors from MRI scans and turn the slice-wise predictions into a 3D visualization. The goal is to go from raw MRI slices → train a MONAI U-Net → get segmentation masks → reconstruct the tumor in 3D so clinicians (or researchers) can inspect the tumor’s size and shape.\n\n\n---\n\n## High-level workflow\n1. **Data processing** — load HDF5 slice files (BraTS2020 Kaggle format), resize to a consistent shape (128×128), normalize intensities, and convert multi-label masks to a single binary tumor mask.  \n2. **Model** — a MONAI U-Net (2D, nnU-Net–style configuration) with residual blocks and modest capacity so it trains well on Kaggle GPUs.  \n3. **Training** — combined Cross-Entropy + Dice loss, Adam optimizer, LR scheduler. Train/validation split = 90/10.  \n4. **Evaluation** — monitor validation loss and Dice coefficient during training.  \n5. **3D reconstruction** — stack predicted 2D masks for one patient to form a 3D volume, extract a surface mesh with marching cubes, and show it with PyVista (or save as `.stl`).  \n6. **Outputs** — saved model weights (`.pth`), PNGs comparing prediction vs ground truth, 3D `.npy` arrays and optional STL meshes.\n\n---\n\n## Important design choices (short, plain language)\n- **2D vs 3D:** We use a 2D slice-based model because Kaggle GPUs have limited memory. 2D models are faster to train and let us use larger batches; the trade-off is they don't see the slice-to-slice context that a true 3D model would.  \n- **Binary mask:** BraTS labels multiple tumor sub-regions (necrotic core, edema, enhancing tumor). For simplicity we merge them into a single \"tumor present\" label. You can easily extend to multi-class later.  \n- **Resolution:** Images are resized to **128×128** to speed up experiments. This reduces fine detail; for final results you might want to go higher or use patch-based training.  \n- **Model size:** The chosen U-Net has ~0.5M parameters — small enough for quick training but expressive enough to learn useful features.\n\n---\n\n## What to expect (practical)\n- **Run time:** Training for ~10 epochs typically takes ~1–2 hours on a Kaggle GPU (depends on batch size & exact GPU).  \n- **Typical results:** Validation Dice scores often land around **0.75–0.85** after a good run; results vary by random seed, data split, and hyperparameters.  \n- **Common errors:** small false positives in noisy regions, and missed tiny/diffuse tumor parts. Post-processing like connected component filtering helps.\n\n---\n\n## Generated files & artifacts\n- `model_best.pth` — trained model weights (PyTorch).  \n- `train_loss.png`, `val_dice.png` — training curves.  \n- `predictions/` — sample prediction PNGs with MRI / GT / Pred.  \n- `volumes/volume_XXX.npy` — stacked 3D arrays for MRI and predicted mask.  \n- `meshes/volume_XXX_tumor.stl` — optional exported 3D mesh.\n\n","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade --force-reinstall numpy scipy scikit-learn\n\nimport os\nimport glob\nimport h5py\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import Adam\n\n\nfrom monai.networks.nets import UNet\nfrom monai.losses import DiceLoss\n\n\nfrom sklearn.model_selection import train_test_split\n\n\nfrom skimage import measure\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(f\" Using device: {device}\")\nif torch.cuda.is_available():\n    print(f\" GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\" GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T07:18:29.416073Z","iopub.execute_input":"2025-11-21T07:18:29.416391Z","iopub.status.idle":"2025-11-21T07:18:59.407290Z","shell.execute_reply.started":"2025-11-21T07:18:29.416370Z","shell.execute_reply":"2025-11-21T07:18:59.406276Z"}},"outputs":[{"name":"stdout","text":"Collecting numpy\n  Downloading numpy-2.3.5-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting scipy\n  Downloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (62 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting scikit-learn\n  Downloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\nCollecting joblib>=1.2.0 (from scikit-learn)\n  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\nCollecting threadpoolctl>=3.1.0 (from scikit-learn)\n  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\nDownloading numpy-2.3.5-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scipy-1.16.3-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.9/35.9 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading scikit_learn-1.7.2-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading joblib-1.5.2-py3-none-any.whl (308 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.4/308.4 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\nInstalling collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n  Attempting uninstall: threadpoolctl\n    Found existing installation: threadpoolctl 3.6.0\n    Uninstalling threadpoolctl-3.6.0:\n      Successfully uninstalled threadpoolctl-3.6.0\n  Attempting uninstall: numpy\n    Found existing installation: numpy 2.2.6\n    Uninstalling numpy-2.2.6:\n      Successfully uninstalled numpy-2.2.6\n  Attempting uninstall: joblib\n    Found existing installation: joblib 1.5.2\n    Uninstalling joblib-1.5.2:\n      Successfully uninstalled joblib-1.5.2\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.15.3\n    Uninstalling scipy-1.15.3:\n      Successfully uninstalled scipy-1.15.3\n  Attempting uninstall: scikit-learn\n    Found existing installation: scikit-learn 1.2.2\n    Uninstalling scikit-learn-1.2.2:\n      Successfully uninstalled scikit-learn-1.2.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 2.3.5 which is incompatible.\nnumba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\ndatasets 4.4.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\nydata-profiling 4.17.0 requires numpy<2.2,>=1.16.0, but you have numpy 2.3.5 which is incompatible.\nydata-profiling 4.17.0 requires scipy<1.16,>=1.4.1, but you have scipy 1.16.3 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\npreprocessing 0.1.13 requires nltk==3.2.4, but you have nltk 3.9.2 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.2.3 which is incompatible.\ngoogle-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\ngoogle-colab 1.0.0 requires tornado==6.4.2, but you have tornado 6.5.2 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nopencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\nopencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\nopencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.33.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed joblib-1.5.2 numpy-2.3.5 scikit-learn-1.7.2 scipy-1.16.3 threadpoolctl-3.6.0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/1640006553.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0m_distributor_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m )\n\u001b[0;32m---> 73\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m  \u001b[0;31m# noqa: E402\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInconsistentVersionWarning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metadata_requests\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_MetadataRequester\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_routing_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_missing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_scalar_nan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'InconsistentVersionWarning' from 'sklearn.exceptions' (/usr/local/lib/python3.11/dist-packages/sklearn/exceptions.py)"],"ename":"ImportError","evalue":"cannot import name 'InconsistentVersionWarning' from 'sklearn.exceptions' (/usr/local/lib/python3.11/dist-packages/sklearn/exceptions.py)","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/brats2020-training-data/BraTS2020_training_data/content/data\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T07:15:29.288894Z","iopub.status.idle":"2025-11-21T07:15:29.289243Z","shell.execute_reply.started":"2025-11-21T07:15:29.289074Z","shell.execute_reply":"2025-11-21T07:15:29.289090Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"h5_files = sorted(glob.glob(os.path.join(DATA_PATH, \"*.h5\")))\nprint(f\" Total .h5 files found: {len(h5_files)}\")\n\n\nprint(\" First 10 files:\")\nfor i, f in enumerate(h5_files[:10]):\n    print(f\"  {i+1}. {os.path.basename(f)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T07:15:29.293715Z","iopub.status.idle":"2025-11-21T07:15:29.294032Z","shell.execute_reply.started":"2025-11-21T07:15:29.293862Z","shell.execute_reply":"2025-11-21T07:15:29.293884Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample_file = h5_files[100]  # Pick a middle file for variety\nprint(f\" Examining file: {os.path.basename(sample_file)}\")\n\nwith h5py.File(sample_file, 'r') as f:\n    # Display all keys stored in the HDF5 file\n    print(f\" Keys in file: {list(f.keys())}\")\n    \n    # Extract image (MRI slice) and segmentation mask\n    image = f['image'][:]\n    mask = f['mask'][:]\n    \n    print(f\" Image shape: {image.shape}, dtype: {image.dtype}\")\n    print(f\" Mask shape: {mask.shape}, dtype: {mask.dtype}\")\n    print(f\" Unique mask values: {np.unique(mask)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T01:37:12.790162Z","iopub.execute_input":"2025-11-21T01:37:12.790923Z","iopub.status.idle":"2025-11-21T01:37:12.819447Z","shell.execute_reply.started":"2025-11-21T01:37:12.790895Z","shell.execute_reply":"2025-11-21T01:37:12.818649Z"}},"outputs":[{"name":"stdout","text":" Examining file: volume_100_slice_5.h5\n Keys in file: ['image', 'mask']\n Image shape: (240, 240, 4), dtype: float64\n Mask shape: (240, 240, 3), dtype: uint8\n Unique mask values: [0]\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n# Plot MRI image (use first channel if multi-channel)\nif len(image.shape) == 3:\n    img_display = image[:, :, 0]\nelse:\n    img_display = image\n\naxes[0].imshow(img_display, cmap='gray')\naxes[0].set_title('MRI Slice (T1/FLAIR)', fontsize=14)\naxes[0].axis('off')\n\n# Plot segmentation mask\naxes[1].imshow(mask, cmap='viridis')\naxes[1].set_title('Segmentation Mask', fontsize=14)\naxes[1].axis('off')\n\n# Overlay mask on MRI for better visualization\naxes[2].imshow(img_display, cmap='gray')\naxes[2].imshow(mask, cmap='hot', alpha=0.4)\naxes[2].set_title('Overlay', fontsize=14)\naxes[2].axis('off')\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T01:52:01.740555Z","iopub.execute_input":"2025-11-21T01:52:01.740863Z","iopub.status.idle":"2025-11-21T01:52:02.087779Z","shell.execute_reply.started":"2025-11-21T01:52:01.740832Z","shell.execute_reply":"2025-11-21T01:52:02.087058Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 3 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAABcYAAAH/CAYAAAB9zg7OAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAABUtklEQVR4nO3deZxddX0//vdd585MJpkkECBAwh5QLIoIgoCC/BrQB0JRqrQqFkRxK4hLi4qAaFErFfeirG4oLqBApSBFRVEsolUpRGVJIEDIPvvc7fz+4HtOZ5gEEkgyIef5fDzuI5lzzz33cyaT+dzzOp/P+1NIkiQJAAAAAADIieJkNwAAAAAAADYlwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAADxLPPDAA1EoFOJNb3rTZDdls3fOOedEoVCIn/zkJ5PdFAByRn8Nzw6CcVgHb3rTm6JQKMQDDzyQbdscO7prrrkmCoVC3HbbbZPdlC3eihUrYtq0afH+979/spsCsEEMDg7Gv/zLv8S+++4bU6ZMiY6Ojthhhx3ikEMOiTPPPDPuvffeyW7iZuvyyy+PQqEQl19++QY53k477RQ77bTTBjnWxpZ+HioUCrHttttGs9lc43533313tt+z5dwA2Lzccsst8drXvjZ23HHH6OjoiBkzZsTBBx8cn/70p2NkZGSymwc8CwnG2WA2xIVRemE59tHZ2Rl77LFHvOtd74pHH310jcfcaaedolarrXNbm81mfP7zn48DDzwwpk2bFtVqNbbbbrs44IAD4t3vfnf89re/XedjbS4ajUa8//3vj/nz58dBBx0UETHhe/lUj9TXv/71eOtb3xr77bdfdHR0rNfF/t577x377bdfRIz/mVjbY9WqVdlr1/ffMTU4OBhTp06NQqEQ73jHO9a6X9qeI488ctz2dETZ2EdXV1fsvffe8cEPfjD6+vomHGvGjBnxj//4j/HZz342Fi5cuN5tBtic9Pf3x0EHHRQf/OAHo7+/P17/+tfHe9/73njlK18ZAwMD8fGPfzxuueWWyW4mEbH99tvH3XffHeeff/5kN2WccrkcS5Ysif/4j/9Y4/OXXHJJFIvFKBZdfgCwfprNZrz1rW+Nww8/PK6//vp48YtfHGeccUa87nWvi0cffTTOOOOM2GeffeIvf/nLZDcVeJYpT3YD2PKMvTB61ateNeH59MLoybz85S+Pgw8+OCIili9fHjfffHN8/vOfj2uuuSbuvPPO2HrrrZ92+1qtVhx11FHx4x//OGbPnh3HH398bLPNNrFq1aq4884747Of/Wx0d3fHC17wgic9TnphOm3atKfdlg3pa1/7Wvz5z3+Of//3f8+2nX322RP2u/DCC2P16tVrfC71oQ99KBYuXBhbbbVVbLfddusc/N57771x1113xXnnnTdu+6677hqvf/3r1/iapxOEP9FVV10V/f39USgU4pvf/GZccMEFT+u4r371q2PvvfeOiMh+hv/lX/4lrrvuuvj1r38dHR0d4/Y//fTT4xOf+ER89KMfja985SvP+DwAJsuFF14Yv//97+PNb35zfPnLXx53szQi4v7774/R0dFJah1jVSqV2HPPPSe7GRMcdNBB8T//8z9x6aWXTvj812w24+tf/3occcQR8dOf/nSSWgjAs9WZZ54ZX/7yl+NFL3pRXH311bH99ttnz7VarfjIRz4SH/nIR+LII4+MO++8M6ZOnTqJrQWeVRLYQO6///4kIpJDDz00mTZtWnLMMcdM2KfRaCTbbLNN8td//ddJR0dHMnfu3HHPX3bZZUlEJOeff/647a1WK3nFK16RRETy4Q9/eMJx586dm3R0dKxTO7/61a8mEZEceeSRSb1en/D8I488kvzmN78Zt+3EE09MIiK5//771+k9JsN+++2X7Ljjjkm73X7S/ebOnZs81X/9m266KXnggQeSJEmS888/P4mI5LLLLnvKNlxwwQVJRCS///3vkyT5v5+J+fPnr9M5rM+/41gveclLknK5nJx++ulJRCTf+MY31rjf2tpz9tlnJxGRXHnlleO2Dw8PJ/vss08SEcmll166xmO+6lWvSrq7u5PVq1evd7sBNhdHHXVUEhHJb3/72/V63ZIlS5LTTz892XXXXZNqtZrMnDkzOe6445I//OEPa9z/Jz/5SXLIIYckXV1dyYwZM5K//du/TRYtWpS89KUvndA3pb+bb7nlluTSSy9N9t5776RWqyU77bRT8pnPfCZJkiRpt9vJpz71qWSPPfZIOjo6kt122y254oor1vjeo6OjyQUXXJC84AUvSLq6upIpU6YkBx98cPKDH/xgwr5pv3/fffcln/nMZ5J58+Yl1Wo1mTNnTnLOOeckrVZrwr5reqTuuOOO5B3veEfy3Oc+N5k6dWpSq9WSvffeOzn//PPHfRZJ+6k1Pc4+++xx+5x44okT2v3AAw8kJ510UjJ79uykUqkk22+/fXLSSSclCxcunLBv+j2v1+vJ2WefncydOzepVqvJ7rvvnnzhC19Y4/dwTcb2rW9961uTcrmcLFmyZNw+V199dRIRybe+9a01fv5bvHhx8uEPfzg54IADkq233jqpVqvJ3Llzk7e97W0TjpUkSbJq1arkrLPOSvbaa6+ku7s76enpSXbdddfkjW98Y/b5JUnG/wyN9Yc//CHZfvvtk97e3uTWW29d53MFYNNbsGBBUiwWkxkzZiSPPvroWvf7u7/7uyQikrPOOitJkiTZddddkylTpiSDg4Nr3P/oo49OIiJZsGDBuO3XXHNNcvjhhye9vb1JR0dH8tznPjf513/916TZbI7bL80uLrvssuSHP/xhctBBByVTpkzJ+ri19dfr+pmg1Wolc+bMSWbMmJGMjIys8RwOOeSQpFQqJQ8++OBavy/AkzOXkQ2us7MzXve618X1118fjz322LjnrrvuuliyZEmcdNJJ63XMYrGY1fL+zW9+84za98tf/jIiIt761rdGpVKZ8Py2224b++6771Me58lqjPf398e5554bf/VXfxVdXV0xbdq0eMELXhBnnXVWNBqNcfvef//98eY3vznmzJkTHR0dsd1228Wb3vSm9SrP8cc//jHuuOOOePWrXz1hlN/TccQRR8TcuXPX+3U/+MEPYuedd47nPe95z7gN62rBggXxi1/8Io488sh497vfHYVCIS655JINcuxarRZ///d/HxFr/7n727/92xgcHIzvfOc7G+Q9ASbDzJkzIyLiT3/60zq/5t57740XvvCFceGFF8auu+4a73rXu+IVr3hF3HDDDfHiF784br/99nH733jjjXHEEUfEr3/963jNa14Tb3nLW2LhwoVx8MEHjyur9UQXXnhhvPvd745999033vKWt0Sj0YjTTjstLr744njnO98Z//qv/xqHHHJInHTSSbF8+fI48cQT42c/+9m4Y4yOjsb8+fPjPe95TyRJEieffHK8/vWvj4ULF8YxxxwTn//859f43u973/vivPPOiwMPPDBOPfXUiHi8/NZZZ52V7XPsscfGMcccExERxxxzTJx99tnZI/WVr3wlrr766nje854Xb33rW+Pkk0+OJEnizDPPjNe97nXZfr29vXH22WfHtGnTYtq0aeOO9bKXvexJ/z3+9Kc/xYte9KK49NJL44UvfGG85z3viRe84AVx6aWXxn777bfWf9sTTjghLr300pg/f36cfPLJsWLFinjHO97xtGZCnXTSSdFsNuNrX/vauO2XXnppzJgxI4499tg1vu5nP/tZXHDBBbHNNtvECSecEO9617ti1113jS996Utx4IEHxurVq7N9kySJ+fPnx3nnnRczZsyIt7zlLfGWt7wlXvCCF8QPf/jD+POf//ykbfz5z38ehxxySERE3HrrrdkMRQA2T1dccUW02+14y1veEttss81a90v75ksvvTQiIl7/+tfHwMBAXHPNNRP2XbZsWdxwww1xwAEHxB577JFtP/PMM+PYY4+NBQsWxHHHHRdvf/vbo7OzM973vveN66/H+s53vhPHHXdczJo1K97+9rfHUUcd9aTns66fCYrFYrz5zW+OFStWxPe+970Jx1mwYEHceuutceSRR8YOO+zwpO8JPInJzeXZkowdMXT77bcnEZF86lOfGrfP0Ucfnd3xXJ8R40mSJFdddVUSEWscib4+I40/9KEPJRGRfOITn1jnc1vTiPG13QFesmRJsueeeyYRkTz/+c9PzjjjjOT0009PjjzyyKRSqSQrV67M9v3Vr36VTJs2LSmXy8mxxx6bvO9970uOP/74pFwuJ7NmzUruvffedWrfhRdemI3EeirrMmJ8rHUdMb506dKkVColp59+erZtU4wYf9/73pdERHLVVVclSZIkhx12WFIoFJL77rtvwr7rO2I8SZLkk5/8ZBIRyWmnnbbG97/33nuTiEhOOOGE9Wo3wObkBz/4QRIRSU9PT/Ke97wn+c///M9k2bJlT/qagw46KCmVSskNN9wwbvuCBQuSnp6e5HnPe162rdlsJnPnzk0KhcKEEbpvfOMbJ4ywTpL/+908Y8aMcf3hokWLkmq1mkybNi3ZY489ksceeyx77le/+lUSEcnRRx897lgf+MAHslFkY2dW9fX1Jfvtt19SrVaTxYsXZ9vTfn/nnXdOHn744Wz70qVLk97e3qSnpycZHR3Nto8dNbYmCxcunDDSrN1uJyeddFISEcnPf/7zcc/NnTt3wmek1No+fxx22GFJRCQXXXTRuO1f+MIXkohIDj/88HHb0xHjBxxwwLhZT/fcc09SLpeTefPmrfH919aetG/de++9k+c+97nZ84888khSLpeTd73rXUmSJGv8/LdkyZKkv79/wrGvuOKKJCKSj370o9m23//+90lEJMcee+yE/UdGRsYd54kjxn/wgx8knZ2dybx589Y4ih6Azc/LXvayJCKSm2666Sn3nT17dhIRyaJFi5I///nPSUQkRx111IT9Pve5zyURkXz+85/Ptt14441ZfzYwMJBtb7fbyamnnppERPLd73432572/cVicY1tW1t/vT6fCRYvXpyUy+XkZS972YTjv/e9700iIrnmmmue8vsCrJ0R42wU+++/f+y9995x2WWXZdseffTR+NGPfhR///d/P6FW81Npt9vZsZ7pyJ6/+Zu/iXK5HGeddVaceuqpce2118YjjzzyjI451tvf/va455574gMf+ED89re/jQsuuCA+/elPx49+9KN48MEHY8qUKRHx+GKZr3vd66Ldbsevf/3ruPrqq+OTn/xkXHXVVXHLLbfE8uXL47TTTlun9/zFL34REREvfOELN9h5rK/rrrsuWq3WGkeD/eUvf4lzzjlnwuNXv/rVM3rPZrMZX/3qV6O3tzeOPvroiIh4wxveEEmSZCMFnomRkZH4xje+ERFr/7nbZZddYvr06dm/AcCz0ate9aq44IILIkmSuOCCC2L+/Pmx1VZbxW677RbvfOc7J4zC/e1vfxu33XZbnHjiiTF//vxxz+2xxx5xyimnxB/+8If44x//GBGPj9JduHBhHH300RN+n370ox+NUqm01raddtppscsuu2Rf77jjjnHwwQfH6tWr44Mf/OC4dUcOOOCA2GWXXeJ//ud/sm3tdju+9KUvxa677hrnnnvuuJlVPT098eEPfzjq9Xp8//vfn/DeZ511Vmy33XbZ11tttVUcc8wx0d/fHwsWLFhrm59ozpw5E85x7ILRP/7xj9f5WGuyaNGiuOWWW+I5z3lOnHLKKeOeO/XUU2PPPfeM//qv/4oHH3xwwmvPP//8cbVY582bFy95yUtiwYIF0d/fv95tOemkk+Kuu+7KZgxcccUV0Ww2n3S24KxZs7LPR2O94Q1viKlTp67x+9PZ2TlhW0dHxxqPE/H4GjfHHXdcPO95z4uf//znMWfOnHU9JQAm0aOPPhoRj/f/TyXd55FHHonddtstDjzwwLjpppsmzGT/2te+FpVKJV772tdm29LZY1/+8peju7s7214oFOLjH/94FAqFuPLKKye85zHHHBNHHHHEOp/P+nwmmD17dhx99NHx05/+dNzCoo1GI7761a/GdtttF6985SvX+b2BiSy+yUZz0kknxRlnnBG33357HHDAAet0YZT68Y9/HCMjIxERsWLFivjxj38cd999dxx00EHxtre97Rm1a999940rrrgiTjvttLjooovioosuioiIHXbYIY444oh45zvf+bQD5kcffTS+//3vx6677hrnnHPOhOfHTv267rrr4oEHHoiPfOQjExb6PPjgg+OYY46Ja665Jvr6+p5y8ZCHHnpowvE3tR/84Acxc+bMNQbI9957b5x77rkTtvf29saLX/zip/2eaWmeU045JVts8zWveU28853vjMsvvzzOPffcp1zodazvfve7cc8990RExGOPPRbXX399LFq0KP7mb/4mjjvuuLW+bptttom//OUvkSTJBillAzAZzjjjjDjllFPihhtuiNtuuy3uuOOOuP322+MLX/hCXHLJJfHtb387W1QxvbG5ZMmSNfZ36e/Se+65J/bee+8sqF5TH7HjjjvGnDlz4v77719ju57//OdP2JaG1Wt7bmwZlwULFsTKlStj9uzZa+yLli5dOq7NY63p80A6XfnJyr88Ub1ej89//vPxrW99K+65554YGBiIJEmy5x9++OF1Ptaa/O53v4uIiJe+9KUT+qFisRiHHnpo3HPPPfG73/1uQrDwVOfY09OzXm15/etfH//0T/8Ul156aRxwwAFx2WWXxQte8II1/luN9f3vfz8uuuiiuPPOO2PlypXRarWy58Z+f/baa6/4q7/6q7jyyivjoYceimOPPTZe9rKXxfOf//y19vmf/vSn44c//GHMnz8/vve9740LPADYcr3hDW+IX/7yl3HllVdmg87+/Oc/x69//es4+uijY6uttsr2/dWvfhXd3d1rHWDV2dm5xs8K+++//3q1aX0/E7z1rW+Nq6++Oi6++OL4+Mc/HhERP/zhD+Oxxx6LD3zgA1Eui/XgmfA/iI3m6V4YRUTcfPPNcfPNN4/b9pKXvCRuvvnm9R5tviZ/93d/F8cdd1zcdNNN8fOf/zx+85vfxG233RaXX355fPWrX40vfOELWS3R9XHHHXdEkiRx2GGHrbF++VhpqLBgwYI1hgqPPvpotNvt+NOf/hT77bffkx5r+fLlUSqV1vvidUMZHh6OG2+8MV7zmtescdTf/Pnz44Ybbtjg73vxxRdHRMQb3/jGbFtPT08ce+yx8c1vfjP+8z//8ylrvI31ve99b0L9tuOPPz6+/e1vP2ngPWPGjGg2m7Fq1aqYPn36ep4FwOajp6cnjj/++Dj++OMjImL16tXxgQ98IL74xS/GySefHIsXL45qtRorVqyIiIjrr78+rr/++rUeb3BwMCIi+vr6IuLxkcFrss0226w1GF/TzeH0InBtzzWbzezrtK133XVX3HXXXU/Z1nV977HB7VN5zWteE9dee23sscce8drXvjZmzZoVlUolVq1aFZ/5zGdidHR0nY+1Jun3d203yNMbCel+Y22oc0xtvfXWcfTRR8e3vvWtOP7442PBggXxuc997klfc8EFF8R73/ve2HrrreOv//qvY4cddshGhF944YXjvj/lcjn+67/+K84555z43ve+F+95z3uy933nO98ZH/zgByd8Frn11lsj4vHPI0JxgGeXbbfdNu6555548MEHY968eU+6bzozKu33Xvva18bpp58eX//617NgPF0H4w1veMO4165YsSKazeYab6Kn1vRZYX0Hp63vZ4K//uu/jp133jmuuOKK+OhHPxrlcjkuvvjiKBQKcfLJJ6/XewMTCcbZaJ7OhVHq/PPPj3/+53+OdrsdDzzwQJxzzjnxta99LU455ZT46le/ukHaV6vV4uijj85KcIyMjMSnPvWpOOuss+K0006LY489Nrbddtv1Oma6ONT222//lPumF+ppqY61WVPn+0SdnZ3RarWi0Wg8ZSC/Mdx0000xNDSULT62KTz88MNxww03xC677DJhBOIb3/jG+OY3vxmXXnrpegXjV155Zbzuda+LZrMZCxYsiPe+973xne98J+bNmxfnnXfeWl83PDwcERFdXV1P72QANlPTpk2Lz3/+83H99dfHwoUL4w9/+EO88IUvzMLUz33uc/HOd77zKY+T7v/EqcypJUuWbLhGr+W9X/3qV8d3v/vdjfY+a/Pf//3fce2118b8+fPj+uuvHxfa/upXv4rPfOYzz/g90nNc2/cxnYb+VDPQNpSTTz45vv/978eb3vSmcQtZr0mz2Yzzzjsvtttuu/jd73437uZJkiTxyU9+csJrZs6cGZ/73Ofis5/9bNxzzz3xX//1X/G5z30uzj777KhUKnHmmWeO2/+SSy6Jj33sY3HGGWdEqVSKf/zHf9xwJwvARnXQQQfFT37yk7j55puftGTJPffcEw8//HBsv/322eyoGTNmxCte8Yq45pprYsGCBTFv3rz4+te/HtOmTctygNTUqVOjUCjEsmXL1qt96zNj+Ol8JigUCvGWt7wlzjzzzLj22mtjv/32ixtvvDFe/vKXjys1Bzw9aoyzUZ188snR19e3ThdGa1IsFmOXXXaJK664Ig499ND42te+tsZVpTeEWq0WH/rQh+LQQw+Ner3+tGpG9/b2RkTE4sWLn3Lf9OL02muvjSRJ1vp46Utf+pTHSuurpmH7pvaDH/wgarXahDqzG9Pll18erVYr7rvvvigUCuMeRx55ZEQ8PsVsfT/YRDw+Gu25z31uXH311bHbbrvFxz72sbjzzjvXuv+KFSuip6dng8xmANjcFAqFCaNsDzjggIiI+OUvf7lOx9hnn30iItbYtz700EOxaNGiZ9jKtdtrr71i6tSpcccdd0Sj0dgo75Fe2K5phPW9994bERGvfOUr1zqSeU3HW5/R2ulsvJ/97GfjpmNHPB4u/+xnPxu338Y2f/782H777WPx4sVx7LHHPulsqmXLlsXq1avjwAMPnDCj4I477shuPq9JoVCIvfbaK97xjnfETTfdFBGP9/1PNH369Pjxj38c++23X5x22mkb5GYEAJvGG9/4xigWi/GVr3wlK3+2Jh/72MciIiaUbk1Hhn/961+PX/ziF3H//ffHa17zmqwUZ+qAAw6I5cuXT1hXZUN6Op8JIiL+4R/+ISqVSlx88cVx6aWXRrvdnrCmCPD0CMbZqNbnwujJFAqF+MxnPhOFQiHOPPPMaLfbG7il/2dtizati/322y+KxWLccsstT3nxvb6hwpN53vOeFxGxXguBbSjtdjuuu+66OOKIIzbZ9OSxi2u+6U1vipNPPnnC46CDDop6vZ5NlXs6arVafOpTn4okSeKf//mf17jP4OBgPPTQQ9m/AcCz0UUXXRT//d//vcbnrrnmmrj77rujt7c39t5774h4vJ7mAQccEFdeeWV8+9vfnvCadrsdP/3pT7OvDz744JgzZ05ce+21E/q9s84662mV7FhX5XI53va2t8XChQvjve997xr75z/+8Y9rHc2+LmbMmBERscbFLefOnRsRjy9AOtZdd90V559//lqPt2zZsmy9lacyZ86cOOyww+Kuu+6aUBv1y1/+ctx9991x+OGHr9PCZRtCqVSKa665Jq6++uq1nmNq1qxZ0dnZGXfeeWcMDQ1l21euXBnvete7Juz/wAMPxAMPPDBhezpa/olBR6q3tzduuummeNGLXhSnn356XHjhhet+QgBMmnnz5sVpp50Wy5cvj6OPPjoeeeSRcc+32+0477zz4utf/3rsuuuu8d73vnfc86985Stj+vTp8Y1vfCObff7EMioRkc0mOumkk2L58uUTnn/00Ufj7rvvfkbn8nQ+E0Q8Xq7l2GOPjRtuuCG+9KUvxVZbbRXHHnvsM2oL8DilVNio0gujhx566BmPUnr+858fxx57bFx99dXxjW98Y42d2br41re+FbNmzYrDDjtswrSnX/3qV3HLLbdEuVx+WotCbrPNNvHqV786vvOd78S5554bH/3oR8c9/9hjj8WMGTOiXC7HMcccE3PmzIl/+7d/i/nz58ehhx46bt9GoxG33377Ghcqe6KXvvSl8elPfzpuv/32CcfZ2H75y1/GY489tknLqPz0pz+Ne++9Nw499NC47LLL1rjPggULYs8994xLLrkk3v3udz/t9zrmmGNi3333jZtuuiluvfXWOOSQQ8Y9/5vf/CZardY6jewH2Fz96Ec/ilNPPTV22223eMlLXhKzZ8+OwcHB+O1vfxu33nprFIvF+OIXvzhuZsyVV14Zhx12WLzuda+LCy+8MPbdd9/o7OyMRYsWxS9/+ctYunRpFuyWSqX493//93jVq14Vhx9+eLz2ta+N7bbbLn7605/G4sWLY5999onf//73G+38zj333Ljzzjvjs5/9bFx//fVx6KGHxqxZs2Lx4sXxhz/8If7nf/4nfvnLX661BvpTOfDAA6OzszMuvPDCWLlyZTaT60Mf+lDsv//+sf/++8dVV10VjzzySLz4xS+ORYsWxQ9/+MN45StfucbyLocffnjccccdcdRRR8UhhxwS1Wo1Dj300Cft47/0pS/FwQcfHKecckpce+218ZznPCfuuuuu+OEPfxhbb711fOlLX3pa5/Z07bfffk+5RkrE47MD3/72t8cFF1wQ++yzTxx99NHR19cXP/rRj2Lu3Lkxe/bscfv/7ne/i+OOOy7233//eM5znhPbbrttLF68OK655pooFotP2uen4fj8+fPj3e9+dyRJ8ow+IwCwaXzyk5+M1atXx6WXXhq77757vPKVr4xdd901+vr64sYbb4w///nPsfvuu8d//Md/TCgb1tHREX/7t38bF110UVx22WUxd+7cNfanRx55ZJx11llx3nnnxW677RZHHnlkzJ07N5YvXx5/+ctf4tZbb42PfvSjsddeez3t83g6nwlSp556anznO9+JJUuWxHve856oVqtPux3A/xGMs9Gt64XRujj77LPjmmuuiY985CNxwgknPK0VmNPaXdtvv30ceuihMWfOnKjX63H33XfHjTfeGO12Oz7+8Y+vU53wNfniF78Yf/zjH+NjH/tY/Md//EccfvjhkSRJ/OlPf4obb7wxlixZEr29vdHR0RHf/e5346ijjoqXvvSlcfjhh8fznve8KBQKsXDhwrj11ltj5syZa1z5+ole/vKXR09PT9x0003xvve972m1e6yLL744u4v9hz/8Idv2k5/8JCIeH/n35je/OSIiuxB9Yo22p6vRaMSb3vSmtT5/+eWXxyWXXBIRj08pW5t58+bFQQcdFLfddlvcfvvt2Qj9p+Occ86JV73qVfHhD384brnllnHPpVO33bEHns0+8YlPxEte8pK46aab4mc/+1k2Gmv77bePE088Md71rnfFC1/4wnGv2XnnneO3v/1t/Nu//Vtcc801cdlll0WpVIrtttsuDj300HjNa14zbv+jjjoqbrzxxvjwhz8cV111VXR2dsbLX/7y+Pa3vx2veMUrNmr9646OjvjRj34Ul1xySXz1q1+N733vezE6OhrbbLNNPOc5z4lTTz31Gc38mTFjRnz3u9+Nc845J77yla9k5T8+9KEPRalUiuuuuy7++Z//OW644Yb47//+79h9993jU5/6VBx11FFrvAg+66yzYuXKlXHdddfFrbfeGq1WK84+++wnDcbnzZsXd9xxR5x77rlxww03xPXXXx9bb711/MM//EOcffbZ2Si1zdH5558fM2bMiMsvvzy++MUvxjbbbBMnnHBCnHPOOdkshdR+++0X//RP/xQ/+clP4vrrr49Vq1bFtttuG0cccUS8733ve8qBDdOmTYsbb7wxjjzyyDjjjDOi3W5nC3gCsHkql8txySWXxAknnBBf/vKX4+c//3lcffXV0d3dHXvttVeceuqp8ba3vS1buPmJ3vCGN8RFF10UjUYj/u7v/m6tdcE/8pGPxKGHHhqf/exn4+abb45Vq1bFzJkzY+edd45zzjlnvUvDPtHT+UyQOuyww2LOnDmxaNGi7Foc2AAS2EDuv//+JCKS+fPnr9P+HR0dydy5c8dtu+yyy5KISM4///y1vu7Vr351EhHJJZdckm2bO3du0tHRsU7vu2jRouRzn/tccvTRRye77bZb0t3dnVSr1WTOnDnJ8ccfn9x8880TXnPiiScmEZHcf//92bb0fE888cQJ+69evTo566yzkj333DPp6OhIpk2bljz/+c9PPvzhDyf1en3cvg899FBy2mmnJbvvvnvS0dGRTJ06Ndlrr72SN7/5zWtsy9q87W1vS0qlUvLwww8/6X5z585Nnuq/fnq+a3uMPefdd989Oeigg9Z4nPX9mUjb9mSPVatWJZ2dnUl3d3fS39//pMf7yle+kkREcsoppzxpe84+++wkIpIrr7xyrcfab7/9koiY8G+y8847J89//vPX6fwAmKivry/p7OxM9t9//8luCgDAZunhhx9OyuVycsghh0x2U2CLUkiSJ6zQAzwrLViwIPbee+8455xz4oMf/OAmec+77747nvOc58QnPvGJeP/7379J3nNz8uMf/zj+v//v/4srrrgi3vjGN052cwA2a4ODg9Fut6Onpyfb1mq14h3veEdcdNFF8S//8i9x5plnTmILAQA2T//0T/8Un/zkJ+Ob3/xmnHDCCZPdHNhiCMZhC/K2t70tvvOd78T9998/LnjYWD7+8Y/HmWeeGffcc0/Mmzdvo7/f5uaQQw6JgYGB+M1vfhPForWMAZ7M7373uzj44INj/vz5scsuu0R/f3/ceuut8b//+7/x3Oc+N26//fZNtogzAMDmbvXq1fGlL30pFi5cGBdffHHsscce8fvf/z5KpdJkNw22GIJx2II89thj8cUvfjFe/epXP6NaqTy1FStWxGc/+9k4+uijJ9TdBWCipUuXxvvf//746U9/GkuWLIlmsxlz5syJY489Nj74wQ9Gb2/vZDcRAGCz8cADD8TOO+8ctVotXvziF8e///u/53JAGmxMgnEAAAAAAHLF3H8AAAAAAHJFMA4AAAAAQK4IxgEAAAAAyJXyuu5YKBQ2ZjsAIDc21fIe+m4A2DD03QDw7LIufbcR4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK4JxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMA7lTrVZj6623jqlTp052UwAAAGCLUyqVYsqUKVGr1Sa7KbBWgnEgFwqFQvaoVCrR09MTnZ2dUSwWo1AoZH+mDwAAAODpKZVKUavVolKpZNfYrrXZ3BSSJEnWaUc/vMCzVGdnZ8yePTsKhUK02+0oFApRKpUiIqLdbketVouurq6o1+sxMjISIyMjMTo6GgMDA9Hf3z/JrWdLtI5d7zOm7waADUPfDfDkKpVK9Pb2RsTjvzPHDjpLkiQqlUpUq9VoNpvRbDaj0WhEs9nMrr9hQ1uXvru8CdoBMGkKhUJUq9WYPn16lEqlaLVaWedbLpejq6sruru7o7e3N0ZGRqK/vz+Gh4djYGAgWq1WDA0NRZIk2QMAAAAYL72+LhaL0W63s2vvYrEY1Wo1Ojo6orOzMxqNRoyOjka9Xo/R0dFot9tRr9cjYtPdhISUYBzYYtVqtdhhhx2iVqtFuVyOarUa3d3dUa/XY/Xq1ZEkSTSbzezuddpht9vtaLfbMTo6GiMjI9HX1xerV6+OlStXRl9f32SfFgAAAGwWyuVyTJ8+PbumTq+9W61WDA8PR5Ik0W63I0mSKJVKUSgUshA9SZJs5Pjw8HAMDw/H0NBQjIyMTPZpkROCcWCLlNYS7+3tjXK5HMViMatxViwWY3BwMLsznd6VLpfLUSqVsnrjaQdeLpej3W7H4ODgJJ8VAAAAbD5KpVI2UjxdvysNyUdHR8cNSEv3H7vGV0dHRyRJEsViMZIkyUaPw6YgGAe2OOVyObbaaqvo6uqKarUaxWIxe6T1zCL+r8xKq9WK1atXZx14WvssHTHebDazO9sAAACQd8ViMaZMmRLVanVC2J2WUomIbIR4kiQxPDycrflVKpWiXC5n9cbb7XYUi8VJPivyRjAObHGKxWJ0dXVFZ2dnFoinoXY6hSsixi0GMjo6GuXy//1KLBQK2WKczWYzu/NdKpXGHQMAAADyJh1olo4OHzuQ7InXy+lzac3xsdvTWuRjg/R0BjdsbIJxYIuT1gqvVCpZiD22Uy2VSjFt2rRotVpRr9ejXq/HwMBAlMvl6OjoyBYDabVa0W63s455+vTp0dPTE0uXLo3Vq1dP4hkCAADA5ElHgqeDx8aG2ZVKJQqFQnR2do5biHNkZCQbKZ5ue+Lgte7u7qjVajEwMBDDw8OTeYrkgGAc2KKkZVBqtVpWRiWtFZ4G3WOnbLVarWzaV0RkNc3SeuLpFK+0PnlEZB102okDAABAXqTX1Okjve5OH2lZlHREeFpDfOx1d7PZjNHR0YiIcTO902OOjo5GvV43Y5uNSjAObDFqtVrMmzcvenp6YptttolCoRAjIyMxMjISK1euzDrftBZaWju8t7c3arVa1oGvXLkyIiK6urqiu7t7Qn3yKVOmRKPRiHvvvTeWLVs2yWcNAAAAm0a5XI5tt902arVa9PT0RKFQiEajEY1GI4aGhiIismC8Wq1mf+/s7IxKpZKF5+m+1Wo1Ojo6xtUnb7fbUavVotVqxdKlS2NgYGAyT5ktmGAc2GIUCoVs8cx0scz0Tnb69dgpWmnN8I6Ojpg6dWp2xzq9Q52OPE9f32g0ol6vR0dHR7Tb7ejs7MymgLmDDQAAwJZu7OKZ6Sjv9Np67IjwdN/0+XK5HLVaLdrtdjZgLb1eT+uUF4vFrMRKumBn+pzZ2mwMgnFgizEyMhJ33XVX1Gq1mD17dnR2dsbMmTOjXC7HdtttF0mSRKPRiIiI4eHhrLxKoVCIKVOmRH9/fwwODmblVdIFQqrVanR1dWUddV9fXwwNDcXWW28dlUollixZkpVeAQAAgC1Vo9GIhx9+OCqVSkybNi2q1Wp0d3dna3klSZItpNloNLJSKIVCIWq1WoyMjMTo6GhWXiW97k4Hp6Vh+sjISNTr9ejp6YlSqRR9fX1Rr9cn89TZAgnGgS1Ks9mMRqORLdJRr9ez1bLTO8xj642no73T50qlUhaEp51yeid77B3xJEmiWq1GZ2dnlEqlSTtfAAAA2JTSa+l04FmtVstGko/dJ712TsPx9Lo7vcauVqvZa9Jr7/TPiMdHnpdKpWzUOGxognFgi1GpVGLOnDnZwprNZjP6+vqy6Vnp9K2040078sHBwVi5cmVUq9XYfvvts0565cqVsWLFimzfer0erVYrVq9eHUNDQ1EsFqO7uzvKZb9KAQAA2PKVSqWYMWNGlEqlbMDZ8PBwNsN67CMishHk9Xo9hoaGolwuR29vbxaADw0NZTOw0/ri6THTgW4dHR2CcTYKaQ6wRUunYaXG1j1Lg/L0rnR6xzqtcTYyMhK1Wi06OjqiWq3GyMhI1vGnU8IAAAAgz5543T22RMrYkeBj642nA9IajUZUKpVsENvYEebW82JjE4wDW4x6vR4LFy6MarUa2267bXR0dMRWW22VLaY5tqRKs9mMUqkUtVoturq6Yvr06VlHne7f09MTlUolC8aXLl0a9Xo96vV6jIyMRLPZzBYGAQAAgC1dq9WKFStWZDXFy+VyTJkyJQu6IyJbOLPVakWxWIxKpZKVLE0D87Rcaa1Wy8qllEqlGBgYGHetnQ5OMzCNjUEwDmxRGo1Gdgc67VjH/r1SqYyrJ54uspku4pEkybhpWukd60ajEY1GIyunkk4FazabOmgAAAByo9VqjVuPa+y6XOls7HSk99ia4WMHlaULckY8PsI8DdLTQDwNw9NtRo6zMQjGgS1OWrNs7N3oNBzv7OyMJEmiVqtlnezAwECsWLEi+3rq1KkxY8aMrOPt6+uLkZGRWLp0aVZfvNlsxvDwcIyMjGQLjgAAAEAeFIvF6Orqimq1Om5bOiAtIsYtujk6OhpDQ0PZtlqtFt3d3dl1d3pt3d/fn9UXTwekNRqNaLVak3KebNkE48AWp9VqxeDgYNaJ1mq16OzszO5UF4vFqFar2eKbo6Oj2R3pVqsVo6OjMTAwkHXgIyMjWQmVdGR5OkXMXWsAAADyJg2701KllUolKpXKuJri6cjx9No7HfmdvmZ0dDSSJMlmaaelU9IQ3IKbbGyCcWCLMzIyEvfcc09EPD5ta+utt46tttoqC8VrtVr09vZGs9mMkZGRWLVqVaxevXrcCPFVq1ZlQXg66jwtnZLWIB8dHc1qpgEAAEBeNJvNePTRRyPi8evuKVOmxJQpU7K1u9IZ2+12OxqNRgwNDcXw8HD2+uHh4RgaGspKp6TX3Wk4npZoSQexjV3cEzYUwTiwRRpbu2xsJ5vefU5ronV0dERXV1dMmzYtms1mNBqNGBkZyUaJj46OZnXSnlhLvFKpZAuFAAAAQJ6MvUZ+4t/HLphZLpejWq1mQXk6gjwdfNZsNrMBZ0+clZ2WZjEgjY1BMA5s8dK71RERAwMD0Wg0olQqRWdnZ/T29kZPT09ss802MTAwECtXroylS5fGqlWrsgU3m83muMVDIh7vrNM74itXroyBgYHJOj0AAACYdOmgsbGzqyuVSnR1dUWtVoupU6dmtcbTWuJjB6+tKfzu6OiIjo6OGBoaitHR0U19SmzhBOPAFq9er8fDDz8c1Wo1arVatFqtqFQqWcdbKBQiSZJssY8kSaJSqWS1z9I72hGRlWMpFArZyHKLbwIAAJBnrVYrVq9enY3wbrfbUSqVsjIo6XX32IU00wFo6SMdYZ5ec0fEuNrjsKEVknVcOU4tH+DZrFQqRW9vb8ybNy8rnVIqlaJarWad9NgFQNLOt9lsRl9fX/T19UVHR0eUy+Xszz/96U/x2GOPTSixAk9lUy3aqu8GgA1D3w3w1AqFQnR1dcU222yTlU4pFotRLpez59PFONOwOx2Ilg48K5fL2WtKpVIsWbIk+vv7N9nvYbYc6/IzY8Q4kAtj64qnnW6pVMpGe4/9hTk26E6SJAqFQpTL5SxAH9t5C8UBAAAgslHfaQCerve1ptHeT7yWLhQKWSmVsa9vt9tCcTYawTiQG2nAHRHR39+fdbxpLfF0QZBKpZJN/Uo79Wq1mpVRGR0dzRbqBAAAAB43NuAeGRmJiMdLo4ytI56OBh9bSiUixg1IazQa48qawsYgGAdyI11wc3R0NB577LGs9ll6J7pSqUSj0YhKpRLVanVcJ5xO5yqVStHX1xcDAwNRr9cn+YwAAABg81EsFqNarUaz2cwGpKWLcrZarSiVStmf5XJ53IjwtLZ4sVjMSqsIxtmYBONAblQqlejt7Y0lS5bEX/7ylyiXyzF16tSslmOlUomOjo7sz3K5nN3JTv9eKBSiv78/Fi9ePMlnAwAAAJuXdEBaf39/PPbYY1EsFqOzs3Pc85VKJbvOTgegpQF6Go6PjIzEqlWrJu9EyAXBOJAbQ0NDcd9998Xq1auj1WpFV1dXzJgxI5IkidHR0WxhzXTqVvoYGBiIoaGhmDZtWnR3d6srDgAAAGtQr9dj2bJlMTw8HO12O6rVanR3d2cztdNSKukAtfTP0dHRqNfr0dnZGR0dHeqKs0kIxoHcWL16dfz617/Ovu7q6oo5c+ZEvV6Pvr6+rINuNpuRJElWU3zFihXx0EMPxdy5c7Pa4wAAAMB4w8PD8cADD2RfV6vVmDFjRjSbzRgZGclKpaQlVNIBaYODg7Fq1aqYMWNGlEol191sEoJxILeGhoZi0aJFWQedBuFpB51O4RoYGIiIiL6+vigUCjE0NDTJLQcAAIDNX71ejxUrVkSr1YpGo5FdZ6eLbqbB+OjoaEREFp5b04tNoZCs49yEdGoDAPDMbKppgfpuANgw9N0A8OyyLn13cRO0AwAAAAAANhuCcQAAAAAAckUwDgAAAABArgjGAQAAAADIFcE4AAAAAAC5IhgHAAAAACBXBOMAAAAAAOSKYBwAAAAAgFwRjAMAAAAAkCuCcQAAAAAAckUwDgAAAABArgjGAQAAAADIFcE4AAAAAAC5IhgHAAAAACBXBOMAAAAAAORKebIbAPB0FAqFcV8nSTJJLQEAAADg2UYwDjzrFAqF6O3tjUqlEsViMdrtdixfvjxardZkNw0AAAC2CF1dXVEqlaJQKESSJDEwMGBQGlsUwTjwrFMoFKJarUZHR0cUCoVoNpsTRpADAAAAT0+hUIhSqRSVSiUiItrtdhaQw5ZCMA48a7VarVi1alU0Gg2jxQEAAGADa7fbMTQ0FK1WK9rt9mQ3BzYowTjwrNRut6PVasXo6Gg0m83Jbg4AAABsUZIkiXa7Hc1mUyjOFqmQrOMcCGUKgM1JuVzOyqiYysWzzab6mdV3A8CGoe8G8qhYLEahUDBDm2eldem7jRgHnpWMEgcAAICNxyhxtnTFyW4AAAAAAABsSoJxAAAAAAByRTAOAAAAAECuCMYBAAAAAMgVwTgAAAAAALkiGAcAAAAAIFcE4wAAAAAA5IpgHAAAAACAXBGMAwAAAACQK+XJbgCQD1tvvXVsv/320W63o9VqRalUimKxGNVqNcrlctx3333x6KOPTnYzAQAA4FlpypQp0dvbG0mSRLvdjmKxGIVCIcrlchSLxVi2bFn09fVNdjNhsyEYBzaJAw88MN785jfH0NBQDA0NRVdXV3R2dsZWW20Vvb29cd5558W3vvWtyW4mAAAAPCvtsssucfDBB0e9Xo96vR7VajWq1WpMmTIlOjs74/rrr4877rhjspsJmw3BOLBRbbXVVrHDDjvELrvsElOnTo1KpRKVSiW7cz06Ohp9fX0xa9as2GeffWLRokWxcuXKyW42AAAAPCukI8W33nrr6OzsjFKplM3SjohoNpsxMjISU6dOjR122CFWrFgRQ0NDk9xqmHyFJEmSddqxUNjYbQG2QIcddlicdNJJMXPmzJg1a1YMDw/HyMhIrFq1KlatWhXDw8MxOjoaXV1dUa1W4/LLL49f/OIXk91s2KjWset9xvTdALBh6LuBzdm8efPioIMOiilTpkRPT080Go1oNBoxNDQUw8PDUa/Xo9lsZqVMb7vttrj33nsnu9mwUa1L323EOLBB9fT0xOzZs6O7uzumT58e++67b8yePTtqtVqUSqXo6OiIiIhp06ZFtVqNxx57LIaHh6PVakWj0YhWqzXJZwAAAACbr46Ojujt7Y2Ojo7o6uqKOXPmRG9vbzY7u1x+PO7r7OyMcrkc/f390Wg0sjW/2u32JJ8BbB4E48AGNXv27HjVq14VO++8c+y3337R2dkZU6dOHVfjrLu7OyqVSpTL5fjjH/8YfX19MTg4GI1GI0ZGRib7FAAAAGCz1dvbG/vss09stdVWMXfu3KhUKtHZ2RnNZjOazWaUSqWoVqvR09MTpVIpFi9enI0cTwelAYJxYAPp6emJuXPnxq677hqzZ8+Onp6eGBkZGbcCdrVazVbHjohotVpRq9VixowZ0d/fH4ODg7HtttvGTjvtFEuXLo3BwcFJPisAAADYPHR0dMTMmTNj6623jmnTpkVHR0cWcpdKpez6O0mSrIxEu92OSqUS3d3dMTIyEvV6PaZNmxbDw8PR398f9Xp9Mk8JJpVgHNggtttuuzjmmGNi2223jd133z1arVY89thj0dXVFSMjI9Hb2xszZ86MZrOZ1TtrNBrR09MTc+bMiUcffTQKhULsueee0dvbG7fddptgHAAAAP6fadOmxT777BPTpk2LWbNmRbvdjv7+/qhWq9FsNqOzszOmTJkSrVYrezSbzWxAWjpbe9ttt43Ozs647777YsWKFZN9WjBpBOPABlMsFqNYLEapVIokSbKVsMvlcrTb7WzhzZGRkayuWbvdjmq1mj3Su9wWHgIAAIDxisViFAqFKBaLkSRJdh2efl2v16PRaESz2Yx2u52NHi+Xy9k1enoMyDvBOLBBpR10GohXKpWoVCrRbDZj1apVMTAwEH19fVlHPGXKlOjs7IxarZYtDKKTBgAAgDVLB5ONHZxWKpUmDEhL9+vo6MiuzavVanbN7bqbvBOMA8/I1KlTY/fdd4+ddtopZs2aFVOmTMlWu06SJAYHB2NkZCQ6OjqiVqtFq9WKcrk8rmMuFArRbrej0Whki3S2Wq3JPjUAAACYdLVaLWbNmhUzZ86Mnp6e6OjoiFarlY0GHx0djUajkQ1OS0eSVyqVLDQvFAqRJElWXiWdxQ15JhgHnpEZM2bEy1/+8pg1a1bsuOOOUSwWo16vZ4tsDg8Px8DAQEydOjVmzJgRhUIhW/ijp6cnm+LVarVidHQ0RkZGYnh4WDAOAAAAEdHd3R177rlnTJ06NaZPnx6FQiGazWZERBaMj46ORq1Wi+7u7igUClEqlaJarWYD1NJSps1mM5rNZtTrdcE4uScYB56RWq0Ws2fPjt7e3nF3otMR4BGPr4Kdro49PDwcw8PD0W63szvc7XY7yuVy9PT0RLFYzEacAwAAQN5VKpXo7e2Nrq6urHxKRGQjwNO/p4F4Wmc8vd5Ony8Wi9HR0RGFQiG7Hoc8E4wDz0itVosddtghurq6slC8VCpFo9HIgvG0g65UKrF69epYsWJFNJvNGB0dzWqKl8vlLFx35xoAAAAeVy6XY/r06Vl98IjHF+FstVprDMZbrVYMDg5mZVPG1iJPw3XBOAjGgWdobG3wdFGP7u7ubArX1KlTo7e3NwqFQlZvfHR0NFqtVhSLxWg2m9ld7Ha7HUNDQ9Hf359NCwMAAIA8G1sbPJ2Nna7ZldYb7+rqiojI6o2n19rpjO6xNcnr9XqMjIwoYUruCcaBZyStUZaWPxm7sGa1Ws063hUrVsRjjz0WIyMj0Ww2s/IqrVYrm+KVLtY5MDAgGAcAAID4v2C8WCxmJVHK5XKUSqUol8vZ9fTQ0FD09fVFo9HI6oqn191jR5anNcnN1CbvBOPAM/Loo4/GN7/5zdhxxx1j//33zxbSTO9ipyPIi8VizJw5M6ZOnRqzZs2Kzs7O6OzszALwVatWxeDgYEyZMiVmzpwZy5Ytm+QzAwAAgMnX19cXv/71r2PGjBmx0047ZTOuC4VCFIvFbGBaoVCIKVOmRGdnZ/T09ES1Wo1KpZIF4ENDQzE6Oppdpw8MDEzymcHkEowDz8iyZcviRz/6Uey9997x3Oc+N5rNZgwPD2fPT5s2LQqFQtRqtayGeFoTLSKiUChEoVCIkZGRGBgYiM7Ozpg6dWpUq9XJOB0AAADYrAwMDMRdd90Vs2fPjtmzZ0e73Y56vZ4939nZGVOmTIlKpRKdnZ1RLBajUChkz6fX3Y1GI0ZHR6NarUatVotSqTQZpwObDcE4sEGkU7vSu9blcjlqtVoUi8UYGBiIkZGRGBwczO5Mpx1zV1dXdHZ2Rq1Wi2q1GjvuuGNUq9W4//774+GHH57s0wIAAIDNQro+V1o7PB0tXigUstripVJp3AjyiIhqtZqNHi+VSjF9+vQolUqxfPnyWL169SSfFUwewTiwQSRJMqGMSmdnZ7RarRgeHs467Z6enqhUKtmq2Ol+aV3yrbfeOlu4EwAAAPg/rVYrG+mdhuBJkkSj0YiIx0eHJ0kSpVIpG5CW7pfWJU+vy83UJu8E48AG8fDDD8dVV10Vu+++exx++OFRLpdjypQp2SIgaYdcqVQiImLKlCnZo6urK+vMR0dHY3Bw0OKbAAAAMMaqVaviN7/5TcyaNSv23HPPKBaLUavVJlx3p8F5R0dH9qhWq9n2tKSKxTfJO8E4sEGsWrUq7rjjjoiIOOyww6JUKkVHR0fWMacjxpMkiYjHO+ipU6dmI8UjHr/zXa/XY3h4OFsxGwAAAIgYHh6OhQsXRkTEvHnzslnYETHu2ju97k5naKcjxSMi2u12tFqtaDQagnFyTzAObFDlcjl6e3ujUqnE6tWrsxpoU6dOjd7e3uxO9sDAQAwNDcWSJUti+fLl8cc//jEWLlwYixYtimXLlsWDDz442acCAAAAm51isRhdXV1RKpVieHg4u86u1WrR1dWVBeOjo6NRr9ejr68vBgcH4+GHH47ly5fHihUrYmBgIFauXDnJZwKTSzAObFClUinK5XK02+3o7+/P7lh3d3dn21utVoyMjMTo6Gjcd999cf/998f//u//ZsH4ihUrJvs0AAAAYLOUrtmVJEmMjIxki2x2dHRk29vtdjQajWg2m7Fs2bJYtmxZPPLII1kwPjQ0NMlnAZOvkKS3kZ5qx//3nwzgycycOTN23333iHh8itYhhxwSJ5xwQhSLxSgUCtHf3x+rV6+OpUuXxtKlS2Px4sWxZMmSuOuuu+LBBx+M4eHhqNfrk3wWsHGtY9f7jOm7AWDD0HcDm5Pu7u6YNWtWRDz++2n33XePF73oRdnAtJGRkRgeHo7+/v4YGBiIVatWRV9fXzz88MOxcuXKqNfrypeyxVuXvtuIcWCDWr58eSxfvjz7eu7cuVGtVqPRaMTg4GCsXr06li9fHosXL46HH344li1bFitXroxVq1bF6tWrJ7HlAAAAsPkbHByM+++/P/t65syZUS6Xo9VqxejoaAwPD8fg4GB2nT0wMBCDg4MxNDQUw8PDk9hy2LwYMQ5sVNttt13suuuuERHRbDaj2WxGvV7PSqnU6/VoNBoxMDAQIyMjk9xa2DSMOgOAZxd9N7A5mzZtWmy99dYREdFqtaLdbkez2cxKqbRaraykabPZnOTWwqaxLn23YBwANjEX1wDw7KLvBoBnl3Xpu4uboB0AAAAAALDZEIwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkSnmyGwAwGQqFwoQ/2+12JEkymc0CAACALUp63R0RrrnZrAjGgdyp1Wqx1VZbRalUinK5HJVKJbq6umLZsmWxaNGiyW4eAAAAPKuVy+WYMmVKFIvFKJVKUSqVolqtxsDAQKxYsWKymwcRIRgHcijtoMvlclSr1ajVatHT0xP1ej0qlcqEO9jtdjva7fYktRYAAACeXUqlUtRqtSgWi1Eul6NcLketVotmsxmlUmnCdXeSJEaTs8kJxoHcKZVK0dXVFZVKJWq1WnR0dERnZ2fsuOOOsdVWW0Wz2YxGo5FN93rwwQdj8eLFk9xqAAAAeHYoFotRrVbHzdSuVqsxffr0mDJlSrTb7Wi1Wtn+K1eujFWrVk1eg8klwTiQG4VCIUqlUlQqlXGPtKNOR5KnwXhq5cqVUalUotVqRbvdjlKpFMViMfsaAAAAeNzY8inpo1AoRLFYjI6OjqjVatFqtcYF40NDQ1EqlbK1v9L9rQXGxiQYB3Kju7s75syZE11dXTF16tRxgXjacadBeaVSiZGRkRgeHo4dd9wxenp64sEHH4wlS5bE3LlzY9asWXHvvffG0qVLJ/u0AAAAYLPQ0dERM2bMyMqWFovF7Ho7DbtLpVJ0dHREqVSKRqMRjUYjpk+fHrVaLVasWBH9/f0xc+bM6OnpiaVLl8bAwMBknxZbKME4sMVLR4pXq9Xo6uqKWq2WdcppuZR2ux3FYjEKhUJWezwtp5LWIO/s7Mymf3V2dkapVJrkMwMAAIDNQxp6V6vVqFQqUSwWIyKy6+60jvjYgDwdNZ6u9zW2/Eq1Ws2OARtDIVnH+QjpDzHAs01nZ2dsv/32WceaBuRpEJ4kSbTb7SzwrtVq0dnZmY0YHxgYiMHBwRgdHY1GoxErV66MgYGBGBkZGVdyBdbVppoKqO8GgA1D3w3w5CqVSvT29malU9Lr73RAWhqKjw3OK5VKNmJ8dHQ0RkdHo9lsRqvViqGhoRgZGcm+hvW1Ln23EePAFq9YLEatVotyuRztdnvcI61Z1mw2o1gsRrPZjHq9HhERo6OjUa/Xo9FoRKvVinK5nE316u/vn+SzAgAAgM1DoVDIRomnIfjYEeJJkkSr1YpCoZD9GRHRaDSy8Du9Rk/X9BodHZ3ks2JLJxgHtnhpeZRyuZzdoY54vHxKo9HI6pt1dHREV1dXrF69Oh588MHs9cViMbuT3Wq1LPwBAAAAY4wtj5IkSVYCZeximul1ebVajeHh4Vi5cuW416elVSy4yaYiGAe2eO12O+r1+rhQPCKyu9BpB57esW42m9mK2Gkt8mazOe5ONgAAAPC4dER4RIyrC7626+70On3s9larlQXj7XZ7Us6DfFFjHNjipVO6urq6Ytttt42urq6YMWNGRPxfzamxd7eXLl0aS5YsiY6Ojuju7o6lS5fG8uXLs32bzaZOmmdEnVIAeHbRdwM8tbR++LRp07K1vZ4oDcEHBgair68vyuVydHR0RH9/fwwODma/b40a55lSYxwgHv9lWK/Xo1gsxvDwcBSLxezriMeD7mazmS0QUq/Xszvd6V1rtc0AAABg7VqtVjQajWwk+Nha4u12O1qtVlZD/IkDztK1v2BTEowDuTE6OhqLFy+Orq6uqNfrWW2zvr6+eOSRR6K7uzumTp2alU2JePyO99hpYAAAAMCaNZvNWLVqVVSr1WwAWqlUipGRkVi9enVUq9Xo7OzMyqZERBaWw6YmGAdyI0mSaDabUa/XY3BwMBsdPjQ0FKOjo9HR0RGlUikajUY0Go0YGRmJcrmcheQAAADAk0tHf6cztUulUtTr9Wg2m1Eul7O64+kI83TRTdjUBONA7oyMjMSSJUuyKV1p3am0BvnixYtj9erV0d/fH4VCQT1xAAAAWA/NZjP6+vombK9WqzF16tRYtWpVDA8Px8jISPT19aknzqQQjAO5tKawu9FoxMDAQNTr9XEragMAAADrZ01hd7qGV1pPPEkSoTiTppCs40+f1bGBLV2hUIhSqRTtdtsocTaqTfXBT98NABuGvhtgwykWiwJxNrp1+fkyYhzg/0lrkAMAAAAbh4FobC4s+QoAAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyBXBOAAAAAAAuSIYBwAAAAAgVwTjAAAAAADkimAcAAAAAIBcEYwDAAAAAJArgnEAAAAAAHJFMA4AAAAAQK4IxgEAAAAAyJVCkiTJZDcCAAAAAAA2FSPGAQAAAADIFcE4AAAAAAC5IhgHAAAAACBXBOMAAAAAAOSKYBwAAAAAgFwRjAMAAAAAkCuCcQAAAAAAckUwDgAAAABArgjGAQAAAADIlf8fN6xZb91XZtoAAAAASUVORK5CYII=\n"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"class BratsSliceDataset(Dataset):   \n    def __init__(self, file_paths, target_size=(128, 128)):\n        self.file_paths = file_paths\n        self.target_size = target_size\n    \n    def __len__(self):\n        return len(self.file_paths)\n    \n    def __getitem__(self, idx):\n        import cv2\n        \n        with h5py.File(self.file_paths[idx], 'r') as f:\n\n            image = f['image'][:]\n            mask = f['mask'][:]\n\n        if len(image.shape) == 3 and image.shape[2] == 1:\n             image = np.repeat(image, 4, axis=2) # Duplicate if only 1 channel loaded\n        elif len(image.shape) == 2:\n             image = np.expand_dims(image, axis=2)\n             image = np.repeat(image, 4, axis=2)\n        \n        # Resize all 4 channels\n        resized_channels = []\n        for i in range(min(4, image.shape[2])): # Ensure we only take up to 4\n            channel = cv2.resize(image[:, :, i].astype(np.float32), self.target_size, interpolation=cv2.INTER_LINEAR)\n            \n            # Z-score Normalization \n            mean = np.mean(channel)\n            std = np.std(channel)\n            if std > 1e-6:\n                channel = (channel - mean) / std\n            else:\n                channel = channel - mean\n                \n            resized_channels.append(channel)\n            \n        image = np.stack(resized_channels, axis=0) # Shape (4, H, W)\n        \n        if len(mask.shape) == 3:\n            mask = mask[:, :, 0]\n        \n        # Resize mask using INTER_NEAREST (critical for preserving labels)\n        mask = cv2.resize(mask.astype(np.float32), self.target_size, interpolation=cv2.INTER_NEAREST)\n\n        \n        mask_mapped = np.zeros_like(mask, dtype=np.int64)\n        mask_mapped[mask == 2] = 1   # Edema\n        mask_mapped[mask == 1] = 2   # Non-enhancing tumor core\n        mask_mapped[mask == 4] = 2   # Enhancing tumor\n        \n        # Ensure mask is 2D (H, W)\n        if len(mask_mapped.shape) != 2:\n            raise ValueError(f\"Mask should be 2D, got shape {mask_mapped.shape}\")\n        \n        \n        # image: (4, H, W), mask: (H, W)\n        image = torch.from_numpy(image.copy()).float()\n        mask = torch.from_numpy(mask_mapped.copy()).long()\n        \n        return image, mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T01:57:20.198471Z","iopub.execute_input":"2025-11-21T01:57:20.199033Z","iopub.status.idle":"2025-11-21T01:57:20.205916Z","shell.execute_reply.started":"2025-11-21T01:57:20.199008Z","shell.execute_reply":"2025-11-21T01:57:20.205269Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"test_dataset = BratsSliceDataset(h5_files[:5])\nsample_img, sample_mask = test_dataset[0]\n\nprint(f\"   Image tensor shape: {sample_img.shape} -> Expected (1, 128, 128)\")\nprint(f\"   Mask tensor shape: {sample_mask.shape} -> Expected (128, 128)\")\nprint(f\"   Mask unique values: {torch.unique(sample_mask)}\")\nprint(f\"   Mask dtype: {sample_mask.dtype} -> Expected torch.int64\")\n\nassert len(sample_mask.shape) == 2, f\"Mask must be 2D! Got {sample_mask.shape}\"\n\n\ntrain_files, val_files = train_test_split(\n    h5_files, \n    test_size=0.1, \n    random_state=42,\n    shuffle=True\n)\n\n\nprint(f\"   Training samples: {len(train_files)}\")\nprint(f\"   Validation samples: {len(val_files)}\")\n\n\ntrain_dataset = BratsSliceDataset(train_files)\nval_dataset = BratsSliceDataset(val_files)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T01:58:00.661338Z","iopub.execute_input":"2025-11-21T01:58:00.661853Z","iopub.status.idle":"2025-11-21T01:58:00.739980Z","shell.execute_reply.started":"2025-11-21T01:58:00.661827Z","shell.execute_reply":"2025-11-21T01:58:00.739361Z"}},"outputs":[{"name":"stdout","text":"   Image tensor shape: torch.Size([1, 128, 128]) -> Expected (1, 128, 128)\n   Mask tensor shape: torch.Size([128, 128]) -> Expected (128, 128)\n   Mask unique values: tensor([0])\n   Mask dtype: torch.int64 -> Expected torch.int64\n   Training samples: 51475\n   Validation samples: 5720\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"BATCH_SIZE = 8\nNUM_WORKERS = 2\n\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\nval_loader = DataLoader(\n    val_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=NUM_WORKERS,\n    pin_memory=True\n)\n\nprint(f\"   Training batches: {len(train_loader)}\")\nprint(f\"   Validation batches: {len(val_loader)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T01:58:22.740662Z","iopub.execute_input":"2025-11-21T01:58:22.741412Z","iopub.status.idle":"2025-11-21T01:58:22.747048Z","shell.execute_reply.started":"2025-11-21T01:58:22.741388Z","shell.execute_reply":"2025-11-21T01:58:22.746419Z"}},"outputs":[{"name":"stdout","text":"   Training batches: 6435\n   Validation batches: 715\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"model = UNet(\n    spatial_dims=2,\n    in_channels=1,\n    out_channels=2,\n    channels=(16, 32, 64, 128),\n    strides=(2, 2, 2),\n    num_res_units=2,\n    dropout=0.2\n).to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\" MONAI U-Net Model Built:\")\nprint(f\"   Total parameters: {total_params:,}\")\n\n# Quick test to verify model output shape\nwith torch.no_grad():\n    test_input = torch.randn(1, 1, 128, 128).to(device)\n    test_output = model(test_input)\n    print(f\"   Input shape: {test_input.shape}\")\n    print(f\"   Output shape: {test_output.shape}\")\n\n# %% [markdown]\n# ## 8️⃣ Define Loss, Optimizer, Metrics\n\n# %%\n# Loss functions\n# CrossEntropyLoss expects: pred (B, C, H, W), target (B, H, W)\nce_loss = nn.CrossEntropyLoss()\n# DiceLoss from MONAI expects: pred (B, C, H, W), target (B, 1, H, W)\ndice_loss = DiceLoss(to_onehot_y=True, softmax=True)\n\ndef combined_loss(pred, target):\n\n    # CrossEntropy: pred (B, 2, H, W), target (B, H, W)\n    ce = ce_loss(pred, target)\n    \n    # Dice: needs target as (B, 1, H, W)\n    target_dice = target.unsqueeze(1).float()\n    dice = dice_loss(pred, target_dice)\n    \n    return ce + dice\n\n# Optimizer\noptimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n\n# Learning rate scheduler\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, mode='min', factor=0.5, patience=2, verbose=True\n)\n\ndef compute_dice_score(pred, target, smooth=1e-6):\n    \"\"\"Compute Dice coefficient for evaluation.\"\"\"\n    pred_class = torch.argmax(pred, dim=1)\n    pred_flat = pred_class.view(-1).float()\n    target_flat = target.view(-1).float()\n    intersection = (pred_flat * target_flat).sum()\n    union = pred_flat.sum() + target_flat.sum()\n    dice = (2. * intersection + smooth) / (union + smooth)\n    return dice.item()\n\nprint(\" Loss, optimizer, and metrics defined!\")\n\n\nNUM_EPOCHS = 10\n\ntrain_losses = []\nval_losses = []\nval_dice_scores = []\n\n\nprint(\"=\" * 60)\n\nfor epoch in range(NUM_EPOCHS):\n    # Training phase\n    model.train()\n    epoch_train_loss = 0.0\n    \n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Train]\")\n    \n    for images, masks in pbar:\n        images = images.to(device)\n        masks = masks.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = combined_loss(outputs, masks)\n        loss.backward()\n        optimizer.step()\n        \n        epoch_train_loss += loss.item()\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n    \n    avg_train_loss = epoch_train_loss / len(train_loader)\n    train_losses.append(avg_train_loss)\n    \n    # Validation phase\n    model.eval()\n    epoch_val_loss = 0.0\n    epoch_dice = 0.0\n    \n    with torch.no_grad():\n        for images, masks in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [Val]\"):\n            images = images.to(device)\n            masks = masks.to(device)\n            \n            outputs = model(images)\n            loss = combined_loss(outputs, masks)\n            dice = compute_dice_score(outputs, masks)\n            \n            epoch_val_loss += loss.item()\n            epoch_dice += dice\n    \n    avg_val_loss = epoch_val_loss / len(val_loader)\n    avg_dice = epoch_dice / len(val_loader)\n    val_losses.append(avg_val_loss)\n    val_dice_scores.append(avg_dice)\n    \n    scheduler.step(avg_val_loss)\n    \n    print(f\"\\n Epoch {epoch+1}/{NUM_EPOCHS} Summary:\")\n    print(f\"   Train Loss: {avg_train_loss:.4f}\")\n    print(f\"   Val Loss: {avg_val_loss:.4f}\")\n    print(f\"   Val Dice: {avg_dice:.4f}\")\n    print(\"-\" * 60)\n\nprint(\"\\n Training Complete!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T01:59:31.842170Z","iopub.execute_input":"2025-11-21T01:59:31.842860Z","iopub.status.idle":"2025-11-21T03:16:44.314157Z","shell.execute_reply.started":"2025-11-21T01:59:31.842834Z","shell.execute_reply":"2025-11-21T03:16:44.313123Z"}},"outputs":[{"name":"stdout","text":" MONAI U-Net Model Built:\n   Total parameters: 401,605\n   Input shape: torch.Size([1, 1, 128, 128])\n   Output shape: torch.Size([1, 2, 128, 128])\n Loss, optimizer, and metrics defined!\n============================================================\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10 [Train]: 100%|██████████| 6435/6435 [09:01<00:00, 11.89it/s, loss=0.4654]\nEpoch 1/10 [Val]: 100%|██████████| 715/715 [00:55<00:00, 12.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 1/10 Summary:\n   Train Loss: 0.4955\n   Val Loss: 0.4714\n   Val Dice: 0.2995\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10 [Train]: 100%|██████████| 6435/6435 [06:33<00:00, 16.36it/s, loss=0.3581]\nEpoch 2/10 [Val]: 100%|██████████| 715/715 [00:42<00:00, 16.76it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 2/10 Summary:\n   Train Loss: 0.4708\n   Val Loss: 0.4641\n   Val Dice: 0.3749\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10 [Train]: 100%|██████████| 6435/6435 [06:46<00:00, 15.82it/s, loss=0.5100]\nEpoch 3/10 [Val]: 100%|██████████| 715/715 [00:42<00:00, 16.88it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 3/10 Summary:\n   Train Loss: 0.4673\n   Val Loss: 0.4610\n   Val Dice: 0.3890\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10 [Train]: 100%|██████████| 6435/6435 [06:38<00:00, 16.15it/s, loss=0.5157]\nEpoch 4/10 [Val]: 100%|██████████| 715/715 [00:43<00:00, 16.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 4/10 Summary:\n   Train Loss: 0.4651\n   Val Loss: 0.4602\n   Val Dice: 0.3686\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10 [Train]: 100%|██████████| 6435/6435 [06:28<00:00, 16.57it/s, loss=0.4755]\nEpoch 5/10 [Val]: 100%|██████████| 715/715 [00:42<00:00, 16.89it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 5/10 Summary:\n   Train Loss: 0.4634\n   Val Loss: 0.4579\n   Val Dice: 0.4008\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10 [Train]: 100%|██████████| 6435/6435 [06:26<00:00, 16.64it/s, loss=0.4298]\nEpoch 6/10 [Val]: 100%|██████████| 715/715 [00:42<00:00, 16.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 6/10 Summary:\n   Train Loss: 0.4617\n   Val Loss: 0.4555\n   Val Dice: 0.4329\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10 [Train]: 100%|██████████| 6435/6435 [06:57<00:00, 15.42it/s, loss=0.5001]\nEpoch 7/10 [Val]: 100%|██████████| 715/715 [00:42<00:00, 16.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 7/10 Summary:\n   Train Loss: 0.4601\n   Val Loss: 0.4544\n   Val Dice: 0.4196\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10 [Train]: 100%|██████████| 6435/6435 [06:23<00:00, 16.78it/s, loss=0.5003]\nEpoch 8/10 [Val]: 100%|██████████| 715/715 [00:43<00:00, 16.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 8/10 Summary:\n   Train Loss: 0.4592\n   Val Loss: 0.4526\n   Val Dice: 0.4110\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10 [Train]: 100%|██████████| 6435/6435 [06:37<00:00, 16.18it/s, loss=0.4390]\nEpoch 9/10 [Val]: 100%|██████████| 715/715 [00:45<00:00, 15.80it/s]\n","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 9/10 Summary:\n   Train Loss: 0.4578\n   Val Loss: 0.4525\n   Val Dice: 0.4225\n------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10 [Train]: 100%|██████████| 6435/6435 [07:46<00:00, 13.79it/s, loss=0.5018]\nEpoch 10/10 [Val]: 100%|██████████| 715/715 [00:52<00:00, 13.52it/s]","output_type":"stream"},{"name":"stdout","text":"\n📈 Epoch 10/10 Summary:\n   Train Loss: 0.4573\n   Val Loss: 0.4509\n   Val Dice: 0.4479\n------------------------------------------------------------\n\n Training Complete!\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].plot(range(1, NUM_EPOCHS+1), train_losses, 'b-o', label='Train Loss')\naxes[0].plot(range(1, NUM_EPOCHS+1), val_losses, 'r-o', label='Val Loss')\naxes[0].set_xlabel('Epoch')\naxes[0].set_ylabel('Loss')\naxes[0].set_title('Training & Validation Loss')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].plot(range(1, NUM_EPOCHS+1), val_dice_scores, 'g-o', label='Val Dice')\naxes[1].set_xlabel('Epoch')\naxes[1].set_ylabel('Dice Score')\naxes[1].set_title('Validation Dice Score')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('training_curves.png', dpi=150)\nplt.show()\n\nprint(f\" Best Val Dice: {max(val_dice_scores):.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T06:41:54.911523Z","iopub.execute_input":"2025-11-21T06:41:54.912253Z","iopub.status.idle":"2025-11-21T06:41:54.922030Z","shell.execute_reply.started":"2025-11-21T06:41:54.912225Z","shell.execute_reply":"2025-11-21T06:41:54.921110Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/2491278512.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'b-o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r-o'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Val Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_xlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"],"ename":"NameError","evalue":"name 'plt' is not defined","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"def visualize_prediction(model, dataset, idx=0):\n    \"\"\"Visualize model prediction vs ground truth.\"\"\"\n    model.eval()\n    image, mask = dataset[idx]\n    image_batch = image.unsqueeze(0).to(device)\n    \n    with torch.no_grad():\n        output = model(image_batch)\n        pred = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n    \n    image_np = image.squeeze().cpu().numpy()\n    mask_np = mask.cpu().numpy()\n    \n    fig, axes = plt.subplots(1, 4, figsize=(20, 5))\n    \n    axes[0].imshow(image_np, cmap='gray')\n    axes[0].set_title('Input MRI')\n    axes[0].axis('off')\n    \n    axes[1].imshow(mask_np, cmap='viridis')\n    axes[1].set_title('Ground Truth')\n    axes[1].axis('off')\n    \n    axes[2].imshow(pred, cmap='viridis')\n    axes[2].set_title('Prediction')\n    axes[2].axis('off')\n    \n    axes[3].imshow(image_np, cmap='gray')\n    axes[3].imshow(mask_np, cmap='Greens', alpha=0.3)\n    axes[3].imshow(pred, cmap='Reds', alpha=0.3)\n    axes[3].set_title('Overlay (Green=GT, Red=Pred)')\n    axes[3].axis('off')\n    \n    plt.tight_layout()\n    return fig\n\n\nfor i in [0, 10, 20]:\n    if i < len(val_dataset):\n        fig = visualize_prediction(model, val_dataset, idx=i)\n        plt.savefig(f'prediction_{i}.png', dpi=150)\n        plt.show()\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-21T06:41:43.056990Z","iopub.execute_input":"2025-11-21T06:41:43.057279Z","iopub.status.idle":"2025-11-21T06:41:43.069214Z","shell.execute_reply.started":"2025-11-21T06:41:43.057257Z","shell.execute_reply":"2025-11-21T06:41:43.068112Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_48/688053052.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvisualize_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'prediction_{i}.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'val_dataset' is not defined"],"ename":"NameError","evalue":"name 'val_dataset' is not defined","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"def extract_volume_id(filepath):\n\n    basename = os.path.basename(filepath)\n    parts = basename.replace('.h5', '').split('_')\n    \n    vol_id = None\n    slice_num = None\n    \n    for i, part in enumerate(parts):\n        if part == 'volume' and i+1 < len(parts):\n            vol_id = parts[i+1]\n        if part == 'slice' and i+1 < len(parts):\n            try:\n                slice_num = int(parts[i+1])\n            except:\n                slice_num = 0\n    \n    return vol_id, slice_num\n\nfrom collections import defaultdict\n\nvolume_dict = defaultdict(list)\n\nfor filepath in h5_files:\n    vol_id, slice_num = extract_volume_id(filepath)\n    if vol_id is not None:\n        volume_dict[vol_id].append((slice_num if slice_num else 0, filepath))\n\n# Sort slices within each volume\nfor vol_id in volume_dict:\n    volume_dict[vol_id].sort(key=lambda x: x[0])\n\nprint(f\" Found {len(volume_dict)} unique volumes\")\nprint(f\"   Sample volume IDs: {list(volume_dict.keys())[:5]}\")\n\n# %%\ndef reconstruct_3d_volume(volume_files, model, device, target_size=(128, 128)):\n\n    import cv2\n    \n    slices_mri = []\n    slices_gt = []\n    slices_pred = []\n    \n    model.eval()\n    \n    for slice_num, filepath in tqdm(volume_files, desc=\"Loading slices\"):\n        with h5py.File(filepath, 'r') as f:\n            image = f['image'][:]\n            mask = f['mask'][:]\n        \n        if len(image.shape) == 3:\n            image = image[:, :, 0]\n        \n        # Resize to match training size\n        image = cv2.resize(image.astype(np.float32), target_size, interpolation=cv2.INTER_LINEAR)\n        mask = cv2.resize(mask.astype(np.float32), target_size, interpolation=cv2.INTER_NEAREST)\n        \n        # Normalize\n        if image.max() > 0:\n            image = (image - image.min()) / (image.max() - image.min() + 1e-8)\n        \n        mask = (mask > 0).astype(np.uint8)\n        \n        slices_mri.append(image)\n        slices_gt.append(mask)\n        \n        # Get prediction\n        img_tensor = torch.from_numpy(image.copy()).unsqueeze(0).unsqueeze(0).float().to(device)\n        with torch.no_grad():\n            output = model(img_tensor)\n            pred = torch.argmax(output, dim=1).squeeze().cpu().numpy()\n        slices_pred.append(pred)\n    \n    X_3d = np.stack(slices_mri, axis=0)\n    Y_3d = np.stack(slices_gt, axis=0)\n    P_3d = np.stack(slices_pred, axis=0)\n    \n    return X_3d, Y_3d, P_3d\n\nvol_ids = list(volume_dict.keys())\nselected_vol = vol_ids[0]\n\nprint(f\" Reconstructing volume: {selected_vol}\")\nprint(f\"   Number of slices: {len(volume_dict[selected_vol])}\")\n\nX_3d, Y_3d, P_3d = reconstruct_3d_volume(volume_dict[selected_vol], model, device)\n\nprint(f\" 3D Volume Shapes:\")\nprint(f\"   MRI: {X_3d.shape}, GT: {Y_3d.shape}, Pred: {P_3d.shape}\")\n\n\ndef plot_3d_slices(volume, title=\"3D Volume\"):\n\n    z, y, x = volume.shape\n    \n    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n    \n    axes[0].imshow(volume[z//2, :, :], cmap='gray')\n    axes[0].set_title(f'{title} - Axial')\n    axes[0].axis('off')\n    \n    axes[1].imshow(volume[:, y//2, :], cmap='gray')\n    axes[1].set_title(f'{title} - Coronal')\n    axes[1].axis('off')\n    \n    axes[2].imshow(volume[:, :, x//2], cmap='gray')\n    axes[2].set_title(f'{title} - Sagittal')\n    axes[2].axis('off')\n    \n    plt.tight_layout()\n    return fig\n\n\nplot_3d_slices(X_3d, \"MRI Volume\")\nplt.savefig('mri_3d_slices.png', dpi=150)\nplt.show()\n\nplot_3d_slices(Y_3d, \"Ground Truth Tumor\")\nplt.savefig('gt_tumor_slices.png', dpi=150)\nplt.show()\n\nplot_3d_slices(P_3d, \"Predicted Tumor\")\nplt.savefig('pred_tumor_slices.png', dpi=150)\nplt.show()\n\ndef create_3d_mesh(volume, threshold=0.5):\n\n    binary_vol = (volume > threshold).astype(np.float32)\n    \n    if binary_vol.sum() == 0:\n        print(\" No tumor voxels found!\")\n        return None, None\n    \n    try:\n        verts, faces, _, _ = measure.marching_cubes(binary_vol, level=0.5)\n        print(f\" Mesh: {len(verts)} vertices, {len(faces)} faces\")\n        return verts, faces\n    except Exception as e:\n        print(f\" Error: {e}\")\n        return None, None\n\n\ngt_verts, gt_faces = create_3d_mesh(Y_3d)\npred_verts, pred_faces = create_3d_mesh(P_3d)\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom mpl_toolkits.mplot3d.art3d import Poly3DCollection\n\ndef plot_3d_mesh(verts, faces, title=\"3D Tumor\", color='red'):\n\n    if verts is None:\n        print(\"No mesh to display\")\n        return None\n    \n    fig = plt.figure(figsize=(10, 10))\n    ax = fig.add_subplot(111, projection='3d')\n    \n    mesh = Poly3DCollection(verts[faces], alpha=0.7)\n    mesh.set_facecolor(color)\n    mesh.set_edgecolor('darkgray')\n    mesh.set_linewidth(0.1)\n    \n    ax.add_collection3d(mesh)\n    \n    ax.set_xlim(verts[:, 0].min(), verts[:, 0].max())\n    ax.set_ylim(verts[:, 1].min(), verts[:, 1].max())\n    ax.set_zlim(verts[:, 2].min(), verts[:, 2].max())\n    \n    ax.set_xlabel('X')\n    ax.set_ylabel('Y')\n    ax.set_zlabel('Z')\n    ax.set_title(title, fontsize=14)\n    \n    return fig\n\nif gt_verts is not None:\n    fig = plot_3d_mesh(gt_verts, gt_faces, \"Ground Truth Tumor 3D\", 'green')\n    plt.savefig('gt_tumor_3d.png', dpi=150)\n    plt.show()\n\nif pred_verts is not None:\n    fig = plot_3d_mesh(pred_verts, pred_faces, \"Predicted Tumor 3D\", 'red')\n    plt.savefig('pred_tumor_3d.png', dpi=150)\n    plt.show()\n\n# Save model weights\ntorch.save(model.state_dict(), 'brats_unet_model.pth')\n\n\n# %%\n# Save 3D volumes as numpy arrays\nnp.save('mri_volume.npy', X_3d)\nnp.save('gt_mask_volume.npy', Y_3d)\nnp.save('pred_mask_volume.npy', P_3d)\nprint(\" 3D volumes saved as .npy files\")\n\n\ndef save_mesh_stl(verts, faces, filename):\n\n    try:\n        from stl import mesh as stl_mesh\n        \n        stl_obj = stl_mesh.Mesh(np.zeros(faces.shape[0], dtype=stl_mesh.Mesh.dtype))\n        for i, f in enumerate(faces):\n            for j in range(3):\n                stl_obj.vectors[i][j] = verts[f[j], :]\n        \n        stl_obj.save(filename)\n        print(f\" Saved: {filename}\")\n    except ImportError:\n        # Alternative: save as OBJ format\n        with open(filename.replace('.stl', '.obj'), 'w') as f:\n            for v in verts:\n                f.write(f\"v {v[0]} {v[1]} {v[2]}\\n\")\n            for face in faces:\n                f.write(f\"f {face[0]+1} {face[1]+1} {face[2]+1}\\n\")\n        print(f\" Saved as OBJ: {filename.replace('.stl', '.obj')}\")\n\nif gt_verts is not None:\n    save_mesh_stl(gt_verts, gt_faces, 'gt_tumor.stl')\n    \nif pred_verts is not None:\n    save_mesh_stl(pred_verts, pred_faces, 'pred_tumor.stl')\n\n\n\nprint(\"=\"*60)\nprint(\"\\n Output Files:\")\nprint(\"   • brats_unet_model.pth - Trained model weights\")\nprint(\"   • training_curves.png - Loss/Dice plots\")\nprint(\"   • prediction_*.png - Sample predictions\")\nprint(\"   • *_3d_slices.png - Orthogonal views\")\nprint(\"   • *_tumor_3d.png - 3D visualizations\")\nprint(\"   • *.npy - 3D volume arrays\")\nprint(\"   • *.stl/*.obj - 3D mesh files\")\nprint(f\"\\n Final Validation Dice Score: {max(val_dice_scores):.4f}\")","metadata":{"trusted":true},"outputs":[{"name":"stdout","text":"/bin/bash: -c: line 1: unexpected EOF while looking for matching `\"'\n/bin/bash: -c: line 2: syntax error: unexpected end of file\n","output_type":"stream"}],"execution_count":40}]}